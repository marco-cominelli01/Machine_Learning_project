{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85cf4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0f35a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e87d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(np.sum(pd.isnull(training), axis = 0).tolist()))\n",
    "# There are no NULL values in training dataset\n",
    "print(sum(np.sum(pd.isnull(test), axis = 0).tolist()))\n",
    "# There are no NULL values in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378282b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.iloc[:, 1:]\n",
    "test = test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ed9de",
   "metadata": {},
   "source": [
    "We want to recover from the data loss of the AGA frequency on test samples. Train a regressor which can predict the value of the AGA feature given the remaining ones. Compare different regression algorithms for this task. Since AGA features are missing in test samples, use only the training data for this step and make use of robust evaluation techniques to compare algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kingdom values: \", np.unique(training[\"Kingdom\"]))\n",
    "print(\"DNA type values: \", np.unique(training[\"DNAtype\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c23cf1",
   "metadata": {},
   "source": [
    "#### Function to one-hot encode categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a561ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_feature(pd_data, column_name):\n",
    "    # Retrieve the unique values (the categories) and an index for each sample\n",
    "    # specifying the sample category (values[value_idx] reconstruct the original array)\n",
    "    col_values = pd_data[column_name].to_numpy().astype('<U')\n",
    "    values, value_idx = np.unique(col_values, return_inverse=True)\n",
    "    n_values = values.size\n",
    "    # Create a temporary identity matrix to convert value_idx into one-hot features\n",
    "    onehots = np.eye(n_values) #when you use an array to index another array in NumPy, it selects rows from the indexed array based on the values in the index array. e[a] selects rows from the identity matrix e based on the values in array a\n",
    "    value_onehot = onehots[value_idx]\n",
    "    # Remove the categorical feature\n",
    "    pd_data = pd_data.drop(column_name, axis=1)\n",
    "    # Add the new featues\n",
    "    for i in range(n_values):\n",
    "        pd_data[\"{}_{}\".format(column_name, values[i])] = value_onehot[:, i]\n",
    "        \n",
    "    return pd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a75e3",
   "metadata": {},
   "source": [
    "#### One-hot encoding of both the 'Kingdom' and 'DNAtype' features (on both training and test datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8622e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ohe = onehot_feature(training, 'Kingdom')\n",
    "training_ohe = onehot_feature(training_ohe, 'DNAtype')\n",
    "\n",
    "test_ohe = onehot_feature(test, 'Kingdom')\n",
    "test_ohe = onehot_feature(test_ohe, 'DNAtype')\n",
    "\n",
    "training_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b734d",
   "metadata": {},
   "source": [
    "#### Correlation between AGA and each codon and then between each pair of codon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea32681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's check if we can use Pearson correlation:\n",
    "\n",
    "#for col in training.columns[5:]:\n",
    "#    plt.hist(training.loc[:, col], bins = 10)\n",
    "#    plt.show()\n",
    "    \n",
    "# We can't because they don't follow a gaussian distribution, so we use rank correlation methods like Spearman:\n",
    "\n",
    "from scipy import stats\n",
    "print(\"Codons that have a Spearman correlation >= 0.5 (in absolute terms) with AGA:\")\n",
    "for col in training.columns[5:]:\n",
    "    if col != 'AGA':\n",
    "        res = stats.spearmanr(training.loc[:, col], training.loc[:, 'AGA'])\n",
    "        if abs(res.statistic) >= 0.5:\n",
    "            print(\"{}: {} -- p-value = {}\".format(col, res.statistic, res.pvalue))\n",
    "\n",
    "# Now we look at the Spearman correlation between all codons to understand if there's some redundancy:\n",
    "\n",
    "print(\"\\nPairs of codons with a Spearman correlation >= 0.8 (in absolute terms) NOT considering AGA\")\n",
    "for codon1 in training.columns[5:]:\n",
    "    for codon2 in training.columns[5:]:\n",
    "        if codon1 != codon2 and codon1 != 'AGA' and codon2 != 'AGA':\n",
    "            res = stats.spearmanr( training.loc[:, codon1]    ,  training.loc[:, codon2]    )\n",
    "        if abs(res.statistic) >= 0.8:\n",
    "            print(\"Spearman correlation between {} and {}: {} -- p-value = {}\".format(codon1, codon2, res.statistic,\n",
    "                                                                                     res.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d3dff",
   "metadata": {},
   "source": [
    "#### All functions needed later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61338e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t, f\n",
    "\n",
    "def rss(y_true, y_pred):\n",
    "    y_true = y_true.reshape(y_pred.shape)\n",
    "    return np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "def tss(y):\n",
    "    return np.sum((y - y.mean()) ** 2)\n",
    "\n",
    "def multiple_least_squares(X, y):\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    betas = [model.intercept_, *model.coef_]\n",
    "    return betas, y_pred\n",
    "\n",
    "def show_stats(X, y, betas, names, alpha=None):\n",
    "    n_samples, n_features = X.shape\n",
    "    deg = n_samples-n_features\n",
    "    \n",
    "    if X.shape[1] + 1 == betas.shape[0]:\n",
    "        X = np.concatenate([np.ones([X.shape[0], 1]), X], axis=-1)\n",
    "    \n",
    "    pred = X.dot(betas).reshape(-1)\n",
    "    betas = betas.reshape(-1)\n",
    "    y = y.reshape(-1)\n",
    "    RSE = ((y-pred)**2).sum()/(n_samples - n_features)\n",
    "    \n",
    "    se2_b = RSE*(np.linalg.inv(np.dot(X.T, X)).diagonal())\n",
    "    se_b = np.sqrt(se2_b)\n",
    "    t_stat_b = (betas - 0) / se_b\n",
    "\n",
    "    p_values = np.array([2*t.sf(np.abs(t_stat), deg) for t_stat in t_stat_b])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"Name\"] = names\n",
    "    df[\"Coefficients\"] = betas\n",
    "    df[\"Standard Errors\"] = se_b\n",
    "    df[\"t-stat\"] = t_stat_b\n",
    "    df[\"p-value\"] = p_values\n",
    "    if alpha:\n",
    "        rejectH0 = p_values < alpha\n",
    "        df[\"reject H0\"] = rejectH0    \n",
    "    \n",
    "    RSS = np.sum((y - pred)**2)\n",
    "    MSE = RSS/y.shape[0]\n",
    "    return df\n",
    " \n",
    "def Ftest_restricted(data, y, subset_features):\n",
    "    X_complete = data.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    n = X_complete.shape[0]\n",
    "    \n",
    "    betas_complete, y_pred = multiple_least_squares(X_complete, y)    \n",
    "    rss_complete = rss(y, y_pred)\n",
    "    nf_complete = X_complete.shape[1]\n",
    "    \n",
    "    notS = data.columns.difference(subset_features)\n",
    "    X_restr = data[notS].to_numpy()\n",
    "    betas_restr, y_pred = multiple_least_squares(X_restr, y)\n",
    "\n",
    "    rss_restr = rss(y, y_pred)\n",
    "    nf_restr = X_restr.shape[1]\n",
    "\n",
    "    q = nf_complete - nf_restr\n",
    "\n",
    "    F_num = (rss_restr - rss_complete) / q\n",
    "    F_den = rss_complete / (n - nf_complete - 1)\n",
    "    F = F_num / F_den\n",
    "\n",
    "    p_value = f.sf(F, q, n - nf_complete - 1)\n",
    "    return p_value, F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4405f3",
   "metadata": {},
   "source": [
    "#### Multivariate linear regression models to understand which codon are useless for predicting AGA frequency\n",
    "#### Features considered: Codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0bac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X1 = training.iloc[:, 5:].drop(\"AGA\", axis = 1)\n",
    "y = training.loc[:, \"AGA\"]\n",
    "\n",
    "X1_all_features = [col_name for col_name in X1.columns.tolist()]\n",
    "\n",
    "linear_model1 = LinearRegression(fit_intercept=True)\n",
    "linear_model1 = linear_model1.fit(X1, y)\n",
    "\n",
    "betas = np.array([linear_model1.intercept_, *linear_model1.coef_]).reshape(-1, 1) \n",
    "final_stats = show_stats(X1.to_numpy(), y.to_numpy(), betas, ['Intercept', *X1_all_features], alpha=0.001)\n",
    "print(final_stats)\n",
    "print(\"\\nNOT SIGNIFICANT CODONS:\\n \", final_stats[final_stats.loc[:, 'reject H0']==False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba20c28",
   "metadata": {},
   "source": [
    "#### Multivariate linear regression models to understand which codon are useless for predicting AGA frequency\n",
    "#### Features considered: Codons, Kingdom and DNAtype\n",
    "\n",
    "#### Se consideriamo tutti i livelli della dummy variable \"DNAtype\" la funzione dà NaN per gli SE dei coefficienti --> può aver senso rimuovere i livelli della variabile che hanno troppe poche osservazioni (1 sola) dato che questo porta sicuramente ad una stima non precisa del coefficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = training_ohe.iloc[:, 3:].drop([\"AGA\", \"DNAtype_5\", \"DNAtype_11\"], axis = 1)\n",
    "y = training_ohe.loc[:, \"AGA\"]\n",
    "\n",
    "X2_all_features = X2.columns.tolist()\n",
    "\n",
    "linear_model2 = LinearRegression(fit_intercept=True)\n",
    "linear_model2 = linear_model2.fit(X2, y)\n",
    "\n",
    "betas = np.array([linear_model2.intercept_, *linear_model2.coef_]).reshape(-1, 1) \n",
    "final_stats = show_stats(X2.to_numpy(), y.to_numpy(), betas, ['Intercept', *X2_all_features], alpha=0.001)\n",
    "\n",
    "\n",
    "print(final_stats)\n",
    "print(\"\\nNOT SIGNIFICANT FEATURES: \\n\", final_stats[final_stats.loc[:, 'reject H0']==False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdba9d",
   "metadata": {},
   "source": [
    "#### Performance of the model with only the codons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1cd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "y_predict = linear_model1.predict(X1)\n",
    "print(\"Train R2 score \", r2_score(y, y_predict))\n",
    "print(\"Train MSE score \", mean_squared_error(y, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41e0f1",
   "metadata": {},
   "source": [
    "#### Check that 'DNAtype' and 'Kingdom' features are actually useless to predict AGA frequency\n",
    "By looking at the p-value, we can exclude those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ad4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_exclude = X2.loc[:, \"Kingdom_arc\":].columns.tolist()\n",
    "\n",
    "p_value, F = Ftest_restricted(X2, y, features_to_exclude)\n",
    "print(\"p-value =\", p_value)\n",
    "print(\"F-stat =\", F)\n",
    "if p_value < 0.001:\n",
    "    print(\"Reject H0: There is evidence to say that at least one of the S features is useful\")\n",
    "else:\n",
    "    print(\"Do not Reject H0: There is NO evidence to say that at least one of the S features is useful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766b6bf",
   "metadata": {},
   "source": [
    "## METTERE PLOT SUI RESIDUI PER CAPIRE SE USARE POLINOMIALI ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bbea12",
   "metadata": {},
   "source": [
    "\n",
    "### Ridge vs Lasso\n",
    "#### In theory, Lasso performs better when we expect only few features to be significant. In this case we expect a lot of features to be useful (almost all the codons) so probably Ridge is better (try both but with this premise)\n",
    "#### Don't forget about Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6572c32",
   "metadata": {},
   "source": [
    "#### Ridge, with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "X = training_ohe.iloc[:, 3:].drop(\"AGA\", axis = 1)\n",
    "y = training_ohe.loc[:, \"AGA\"]\n",
    "\n",
    "ridge_linear_model = RidgeCV(alphas=[1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4,\n",
    "                                    1e-3, 1e-2, 1e-1, 1], fit_intercept = True).fit(X, y)\n",
    "ridge_linear_model.score(X, y)\n",
    "ridge_linear_model.alpha_ # Regolarizza poco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a282c",
   "metadata": {},
   "source": [
    "#### Lasso, with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "X = training_ohe.iloc[:, 3:].drop(\"AGA\", axis = 1)\n",
    "y = training_ohe.loc[:, \"AGA\"]\n",
    "\n",
    "lasso_linear_model = LassoCV(cv=10, random_state=0).fit(X, y)\n",
    "lasso_linear_model.score(X, y)\n",
    "for idx,coef in enumerate(lasso_linear_model.coef_):\n",
    "    if abs(coef) != 0:\n",
    "        pass\n",
    "        #print(\"Coefficient of {}: {}\".format(X.columns[idx], coef))\n",
    "    else:\n",
    "        print(\"Coefficient {} was removed\".format(X.columns[idx]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
